{
  "source_file_path_relative_to_docusaurus_root": "docs/11-parser.md",
  "source_file_content_hash": "599b2605fd3d9549bd467dd412df73b6b43ea50662b7347b557c7ddff7528a1c",
  "segments": [
    {
      "segment_id": "76e4a56a",
      "source_content": "# 11. Case Study: Parser",
      "source_content_hash": "95e2ba2f604e2c2ed88b3c1a3f4a75cdffeb759763bf46b8e64234fbb4b9607e",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "# 11. ケーススタディ: パーサー"
      }
    },
    {
      "segment_id": "faebdc62",
      "source_content": "Now with the basic understanding of the MoonBit programming language, we can explore more complex programs and present some interesting cases. In this lecture, we'll present a parser.",
      "source_content_hash": "63d9f4e9f60f4f06bd0c23e82914a10cd3d0875bed4e216430e2d5dabc796693",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "MoonBitプログラミング言語の基本的な理解を得た今、より複雑なプログラムを探求し、いくつかの興味深いケースを紹介します。この講義では、パーサーを紹介します。"
      }
    },
    {
      "segment_id": "e7ec8f99",
      "source_content": "There are various types of languages in the world, including programming languages and other symbolic languages. Let's take the four basic arithmetic operations as an example. For a string like `\"(1+ 5) * 7 / 2\"`, the first step is to split it into a list of tokens. For instance, we can tokenize it into a left parenthesis, integer 1, plus sign, integer 5, right parenthesis, multiplication sign, integer 7, division sign, and integer 2. Although there is no space between integer 1, the parenthesis, and the plus sign, they should be separated into three tokens to follow the lexical rules. This step is known as lexical analysis.",
      "source_content_hash": "907301c5de6f78d075c73e892f2e7ef2b9f3eff8940021fe26fe3a8a5b05f798",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "世界にはさまざまな種類の言語が存在し、プログラミング言語やその他の記号言語が含まれます。四則演算を例にとりましょう。`\"(1+ 5) * 7 / 2\"`のような文字列の場合、最初のステップはそれをトークンのリストに分割することです。例えば、左括弧、整数1、プラス記号、整数5、右括弧、乗算記号、整数7、除算記号、整数2のようにトークン化できます。整数1と括弧、プラス記号の間にスペースがなくても、字句規則に従ってこれらは3つのトークンに分割されるべきです。このステップは字句解析として知られています。"
      }
    },
    {
      "segment_id": "6ef78e84",
      "source_content": "Now with a stream of tokens, we will convert it into an abstract syntax tree (AST) based on syntax/grammar. For example, the sum of integers 1 and 5 should come first, then this sum should be multiplied by 7, and finally, this product should be divided by 2, instead of producing the sum of 1 plus the product of 5 and 7, as this does not follow the syntax rules. This step is known as syntax analysis.",
      "source_content_hash": "4fe0b24c66cc0002ba06a83c02f9ede6db9818d76fda3f30597bcee11d8fb050",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "トークンのストリームを得たら、構文/文法に基づいて抽象構文木（AST）に変換します。例えば、整数1と5の和を最初に求め、その和を7で乗算し、最後にその積を2で除算するべきです。5と7の積に1を加えるような解釈は構文規則に従いません。このステップは構文解析として知られています。"
      }
    },
    {
      "segment_id": "53eaf0e1",
      "source_content": "Lastly, we calculate the final result according to the semantics. For example, semantically, `1 + 5` means to find the sum of integers 1 and 5.",
      "source_content_hash": "17c06111b6ff9fa7861ec2049263105f8bf000be9afd552c00345f8eda2e7d43",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "最後に、意味論に従って最終結果を計算します。例えば、`1 + 5`は整数1と5の和を求めることを意味します。"
      }
    },
    {
      "segment_id": "8ac585fb",
      "source_content": "Syntax analysis/parsing is an important field of computer science because all programming languages will need parsing to analyze and run source code, and thus there exist many mature tools. In this lecture, we will present parser combinators to handle both lexical and syntax analysis. If interested, you may also refer to the recommended readings at the end to dive deeper. All code will be available on the course website. Let's start!",
      "source_content_hash": "367bd115049f224f18efe309fd7a57991299c419e8b04e0de78bc5fcca92f370",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "構文解析/パーシングはコンピュータサイエンスの重要な分野です。すべてのプログラミング言語はソースコードを解析して実行するためにパーシングを必要とするため、多くの成熟したツールが存在します。この講義では、字句解析と構文解析の両方を扱うパーサーコンビネーターを紹介します。興味があれば、最後に紹介する推奨読書を参照してさらに深く学ぶこともできます。すべてのコードはコースウェブサイトで利用可能です。それでは始めましょう！"
      }
    },
    {
      "segment_id": "ba4f48ff",
      "source_content": "## Lexical Analysis",
      "source_content_hash": "31d65e14dbb737f637a052b0b5c3312f33c727f744afc0811f0ea75951ea2d8a",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## 字句解析"
      }
    },
    {
      "segment_id": "cf898102",
      "source_content": "First, let's talk about lexical analysis. It aims to break the input into tokens where the input is a string, and the output is a token stream. For example, \"12 +678\" should be split into integer 12, plus sign, and integer 678. Lexical analysis is in general simple and can usually be done using a finite state machine. Domain-specific languages like lex or flex can automatically generate the program. Here, we will use parser combinators. We first define the lexical rules of arithmetic expressions, including integers, parentheses, operators, and whitespaces.",
      "source_content_hash": "8711146ef6b148a78d2fa4ea2ab9676228e1f1ce1a4544df4a6f3d96b000e290",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "まず、字句解析について話しましょう。その目的は入力をトークンに分割することです。入力は文字列で、出力はトークンストリームです。例えば、\"12 +678\"は整数12、プラス記号、整数678に分割されるべきです。字句解析は一般的に単純で、有限状態機械を使用して行うことができます。lexやflexのようなドメイン固有言語はプログラムを自動生成できます。ここでは、パーサーコンビネーターを使用します。まず、整数、括弧、演算子、空白を含む算術式の字句規則を定義します。"
      }
    },
    {
      "segment_id": "93be3c14",
      "source_content": "```abnf\nNumber     = %x30 / (%x31-39) *(%x30-39)\nLParen     = \"(\"\nRParen     = \")\"\nPlus       = \"+\"\nMinus      = \"-\"\nMultiply   = \"*\"\nDivide     = \"/\"\nWhitespace = \" \"\n```",
      "source_content_hash": "7e99cd531d4b66405738b8ef59ce894d6eceadb8db823a0faa5ca6947f5294c7",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_93be3c14"
      }
    },
    {
      "segment_id": "d9172fa4",
      "source_content": "Let's take integers and the plus sign for examples. Each line in the lexical rules corresponds to a pattern-matching rule. Content within quotes means matching a string of the same content. Rule `a b` means matching rule `a` first, and if it succeeds, continue to pattern match rule `b`. Rule `a / b` means matching rule `a` or `b`, try matching `a` first, and then try matching rule `b` if it fails. Rule `*a` with an asterisk in front refers to zero or more matches. Lastly, `%x` means matching a UTF-encoded character, where `x` indicates it's in hexadecimal. For example, `0x30` corresponds to the 48th character `0`, and it is `30` in hexadecimal. With this understanding, let's examine the definition rules. Plus is straightforward, representing the plus sign. Number corresponds to zero or a character from 1-9 followed by zero or more characters from 0-9.",
      "source_content_hash": "66dabbf12d9f058d5b6b625b493c2962c2691d457bdaa5701695aaa89d658302",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "整数とプラス記号を例にとりましょう。字句規則の各行はパターンマッチング規則に対応します。引用符内のコンテンツは同じ内容の文字列にマッチすることを意味します。規則`a b`は、最初に規則`a`をマッチし、成功した場合に規則`b`をマッチし続けることを意味します。規則`a / b`は、規則`a`または`b`をマッチすることを意味し、最初に`a`を試し、失敗した場合に`b`を試します。アスタリスクが前に付いた規則`*a`は、ゼロ回以上のマッチを指します。最後に、`%x`はUTFエンコードされた文字にマッチすることを意味し、`x`は16進数であることを示します。例えば、`0x30`は48番目の文字`0`に対応し、16進数では`30`です。この理解をもとに、定義規則を確認しましょう。プラスは単純にプラス記号を表します。数字は、1-9の文字がゼロ回以上続き、その後0-9の文字がゼロ回以上続くことを表します。"
      }
    },
    {
      "segment_id": "26524352",
      "source_content": "![](/pics/lex_rail.drawio.webp)",
      "source_content_hash": "9b2fa43e11aa6dc05e72a9e03c6cd6e4ccab86e5240d68117c66c4023ada403f",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "![](/pics/lex_rail.drawio.webp)"
      }
    },
    {
      "segment_id": "73ab96a3",
      "source_content": "In MoonBit, we define tokens as enums, and tokens can be values containing integers or operators and parentheses. Whitespaces are simply discarded.",
      "source_content_hash": "8c902b2e837bface9fd190ce039db11869e7df9a1a49be9c3864ad3dc7b29505",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "MoonBitでは、トークンを列挙型として定義し、トークンは整数を含む値や演算子、括弧を含むことができます。空白は単純に破棄されます。"
      }
    },
    {
      "segment_id": "6543a19e",
      "source_content": "```moonbit\nenum Token {\n  Value(Int); LParen; RParen; Plus; Minus; Multiply; Divide\n} derive(Show)\n```",
      "source_content_hash": "8ddbc8b552f0f5be43f5a5a668a167ef4c7447afc5fb93514ba1e151b2ae99f4",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_6543a19e"
      }
    },
    {
      "segment_id": "782c902a",
      "source_content": "### Parser Combinator",
      "source_content_hash": "c1a911d9c46882c78345bec142331627734a1cf7e4717373af0ed0993718f070",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "### パーサーコンビネーター"
      }
    },
    {
      "segment_id": "50746522",
      "source_content": "We then proceed to build a combinable parser. The parser is a function that takes a string as input and outputs a nullable value `Option[T]`.  When `Option[T]` is an empty value, it indicates the pattern matching failed, while a non-empty value contains a result and the remaining string. Ideally, when parsing fails, we should provide error messages like why and where it failed, but this is omitted for simplicity. Feel free to implement it using `Result[A, B]`. We also provide a `parse` method for convenience.",
      "source_content_hash": "63780d5dbf7893d2240c51f96d300ea8fb001f4a70eeaf97333586c5788fc1f9",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "次に、組み合わせ可能なパーサーを構築します。このパーサーは文字列を入力として受け取り、null許容型の値`Option[T]`を出力する関数です。`Option[T]`が空の値の場合、パターンマッチングが失敗したことを示し、非空の値は結果と残りの文字列を含みます。理想的には、パースが失敗した場合に、なぜどこで失敗したかといったエラーメッセージを提供すべきですが、簡略化のためここでは省略しています。`Result[A, B]`を使用して実装することも自由です。また、利便性のために`parse`メソッドも提供しています。"
      }
    },
    {
      "segment_id": "45022e4f",
      "source_content": "```moonbit\n// V represents the value obtained after parsing succeeds\n// Lexer[V] == (String) -> Option[(V, String)]\ntype Lexer[V] (String) -> Option[(V, String)]\n\nfn parse[V](self : Lexer[V], str : String) -> Option[(V, String)] {\n  (self.0)(str)\n}\n```",
      "source_content_hash": "dfc702376668bf55944b3d95c3e0565b0f757b19139a3477308392cfba549b87",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_45022e4f"
      }
    },
    {
      "segment_id": "b36c6338",
      "source_content": "We first define the simplest parser, which just matches a single character. To construct this parser, we need a function to check if an input character meets the conditions. In line 3, if the input is not empty and the first character meets our criteria, we read that character and return it as the value, along with the remaining string. Otherwise, we return an empty value indicating matching failure. We'll use this parser in the following code. For instance, we define an anonymous function to determine if a character is `a` and use it to parse the string `\"asdf\"`. As \"asdf\" starts with `a`, parsing is successful, giving us the result of `a` and the remaining string `\"sdf\"`. If we use the same function to match the remaining string again, it will fail.",
      "source_content_hash": "647acfc205722a6978dded5f63d2d2345b50be24aabd83a7ca9daa43979c2a45",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "まず、最も単純なパーサーを定義します。これは単一の文字にマッチするだけのものです。このパーサーを構築するには、入力文字が条件を満たすかどうかをチェックする関数が必要です。3行目では、入力が空でなく、最初の文字が条件を満たす場合、その文字を値として読み取り、残りの文字列とともに返します。それ以外の場合は、マッチング失敗を示す空の値を返します。このパーサーは以下のコードで使用します。例えば、文字が`a`かどうかを判定する無名関数を定義し、それを使って文字列`\"asdf\"`をパースします。\"asdf\"は`a`で始まるため、パースは成功し、結果として`a`と残りの文字列`\"sdf\"`が得られます。同じ関数で残りの文字列を再度マッチングさせると、失敗します。"
      }
    },
    {
      "segment_id": "eb80afe1",
      "source_content": "```moonbit\nfn pchar(predicate : (Char) -> Bool) -> Lexer[Char] {\n  Lexer(fn(input) {\n    if input.length() > 0 && predicate(input[0]) {\n      Some((input[0], input.substring(start=1)))\n    } else {\n      None\n} },) }\n\ntest {\n  inspect!(pchar(fn { ch => ch == 'a' }).parse(\"asdf\"), content=\"Some((\\'a\\', \\\"sdf\\\"))\")\n  inspect!(pchar(fn {\n    'a' => true\n    _ => false\n  },).parse(\"sdf\"),content=\"None\",)\n}\n```",
      "source_content_hash": "340637d083f6e78c49342081f41e4be4ee39d241d3ddcdf6a751bbc27294029c",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_eb80afe1"
      }
    },
    {
      "segment_id": "883231d4",
      "source_content": "With this simple parser, we can already handle most tokens, including parentheses, arithmetic operators, and whitespaces. Here, we also define them using anonymous functions and directly try pattern matching all possibilities. It returns `true` if the character is something we want to match; otherwise, it returns `false`. It's the same for whitespaces. However, simply parsing the input into characters isn't enough since we want to obtain more specific enum values, so we'll need to define a mapping function.",
      "source_content_hash": "93a517750645356cb810da42dcd52f6e78c40f27273fd91ab23cd1044f5526d8",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "この単純なパーサーを使えば、括弧、算術演算子、空白など、ほとんどのトークンを処理できます。ここでは、無名関数を使用してこれらを定義し、直接すべての可能性をパターンマッチングしようとします。マッチさせたい文字であれば`true`を返し、そうでなければ`false`を返します。空白についても同様です。しかし、単に入力文字を文字としてパースするだけでは不十分です。より具体的な列挙型の値を取得したいため、マッピング関数を定義する必要があります。"
      }
    },
    {
      "segment_id": "468b36ba",
      "source_content": "```moonbit expr\nlet symbol: Lexer[Char] = pchar(fn{\n  '+' | '-' | '*' | '/' | '(' | ')' => true\n  _ => false\n})\n```",
      "source_content_hash": "8c6bdf64f7f66a5c186431aeba028cb893f32fbb38e476eb42fdc17fc912ecfe",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_468b36ba"
      }
    },
    {
      "segment_id": "092b29bb",
      "source_content": "```moonbit\nlet whitespace : Lexer[Char] = pchar(fn{ ch => ch == ' ' })\n```",
      "source_content_hash": "0625908d58ce9387b2f42bf752307adfb52c7ba078a4a23ba4e30ba4f99ca9ef",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_092b29bb"
      }
    },
    {
      "segment_id": "8126f763",
      "source_content": "The `map` function can convert the result upon successful parsing. Its parameters include the parser itself and a transformation function. After obtaining the value, we apply the transformation function. Utilizing this, we can map the characters corresponding to arithmetic operators and parentheses into their corresponding enum values.",
      "source_content_hash": "eea6d2eef2de5b709f94d9681deba9b673c84e2c88623af3fef45d2ea3f7adf6",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "`map`関数は、パースが成功した際に結果を変換できます。そのパラメータにはパーサー自体と変換関数が含まれます。値を取得した後、変換関数を適用します。これを使用して、算術演算子や括弧に対応する文字を、対応する列挙型の値にマッピングできます。"
      }
    },
    {
      "segment_id": "f6563256",
      "source_content": "```moonbit\nfn map[I, O](self : Lexer[I], f : (I) -> O) -> Lexer[O] {\n  Lexer(fn(input) {\n    // Non-empty value v is in Some(v), empty value None is directly returned\n    let (value, rest) = match self.parse(input) {\n      Some(v) => v\n      None => return None\n    }\n    Some((f(value), rest))\n},) }\n\nlet symbol : Lexer[Token] = pchar(\n  fn {\n    '+' | '-' | '*' | '/' | '(' | ')' => true\n    _ => false\n  },).map(\n  fn {\n    '+' => Token::Plus\n    '-' => Token::Minus\n    '*' => Token::Multiply\n    '/' => Token::Divide\n    '(' => Token::LParen\n    ')' => Token::RParen\n},)\n```",
      "source_content_hash": "cf3c45f864cc2bf870a7c37a1d318b22a1409e2a1ea4460cdee696ec4d791d7e",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_f6563256"
      }
    },
    {
      "segment_id": "e9313fef",
      "source_content": "Let's look at other combinators. We've seen other pattern matching rules like matching `a` followed by `b`, `a` or `b`, zero or more occurrences of `a`, etc. Each combinator is simple to implement, and let's do it one by one. For matching `a` and then `b`, we first use `self` for parsing, as shown in line 3. After obtaining the value and the remaining string, we use another parser to parse the remaining string, as shown in line 7. The two outputs are returned as a tuple. Then, for matching `a` or `b`, we will pattern match the result of parsing using `self`. If empty, we use the result of another parser; otherwise, we return the current result.",
      "source_content_hash": "b9b68acbccc15c412ad846917f17d6b21c8f84e98b3446c09b1bd1d9e9cf28eb",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "他のコンビネーターを見てみましょう。`a`の後に`b`をマッチさせる、`a`または`b`をマッチさせる、`a`の0回以上の繰り返しをマッチさせるなど、他のパターンマッチングルールがあります。各コンビネーターは実装が簡単で、一つずつ進めていきましょう。`a`の後に`b`をマッチさせる場合、まず3行目で示すように`self`を使ってパースします。値と残りの文字列を取得した後、7行目で示すように別のパーサーを使って残りの文字列をパースします。2つの出力はタプルとして返されます。次に、`a`または`b`をマッチさせる場合、`self`を使ってパースした結果をパターンマッチします。空の場合は別のパーサーの結果を使用し、それ以外の場合は現在の結果を返します。"
      }
    },
    {
      "segment_id": "342e4d80",
      "source_content": "```moonbit\nfn and[V1, V2](self : Lexer[V1], parser2 : Lexer[V2]) -> Lexer[(V1, V2)] {\n  Lexer(fn(input) {\n    let (value, rest) = match self.parse(input) {\n      Some(v) => v\n      None => return None\n    }\n    let (value2, rest2) = match parser2.parse(rest) {\n      Some(v) => v\n      None => return None\n    }\n    Some(((value, value2), rest2))\n},) }\n\nfn or[Value](self : Lexer[Value], parser2 : Lexer[Value]) -> Lexer[Value] {\n  Lexer(fn(input) {\n    match self.parse(input) {\n      None => parser2.parse(input)\n      Some(_) as result => result\n} },) }\n```",
      "source_content_hash": "2423b86d4c6c9bd351c38b7968ec11737f35fbd77cd740719342db628b5a3cdd",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_342e4d80"
      }
    },
    {
      "segment_id": "e77cf7c6",
      "source_content": "For matching zero or more occurrences, we use a loop as shown in lines 5 to 10. We try parsing the remaining input in line 6. If it fails, we exit the loop; otherwise, we add the parsed content to the list and update the remaining input. Ultimately, the parsing always succeeds, so we put the result into `Some`. Note that we're storing values in a list, and a list is a stack, so it needs to be reversed to obtain the correct order.",
      "source_content_hash": "b3194b24f008adfe8e5849b2c521a0cc4bb902445fb091a20092c000b3eb90a6",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "0回以上の繰り返しをマッチさせる場合、5行目から10行目に示すようにループを使用します。6行目で残りの入力をパースしようとします。失敗した場合はループを抜け、成功した場合はパースされた内容をリストに追加し、残りの入力を更新します。最終的にはパースは常に成功するため、結果を`Some`に格納します。値はリストに格納されており、リストはスタックであるため、正しい順序を得るには逆順にする必要があることに注意してください。"
      }
    },
    {
      "segment_id": "6e57b94c",
      "source_content": "```moonbit\nfn many[Value](self : Lexer[Value]) -> Lexer[@immut/list.T[Value]] {\n  Lexer(fn(input) {\n   loop input, @immut/list.T::Nil {\n      rest, cumul => match self.parse(rest) {\n        None => Some((cumul.rev(), rest))\n        Some((value, rest)) => continue rest, Cons(value, cumul)\n      }\n    }\n},) }\n```",
      "source_content_hash": "8152255f6056bfa555cc7a0aefe9efc143c08c1de31fe276a20c2164c4a12e36",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_6e57b94c"
      }
    },
    {
      "segment_id": "3e8f45da",
      "source_content": "Lastly, we can build a lexical analyzer for integers. An integer is either zero or starts with a non-zero digit followed by any number of digits. We'll first build three helper parsers. The first parser matches the character `0` and maps it to the number zero. The next two parsers match `1-9` and `0-9`, respectively. Here, we use the ranges of UTF encoding to determine, and since numbers in UTF are ordered from 0 to 9, we calculate the difference between a character's encoding and the encoding of `0` to obtain the corresponding number. Finally, we follow the syntax rules to construct the parser using our combinators. As shown in lines 11 and 12, we mirror the rules exactly. However, a non-zero digit and any number of digits just form a tuple of a digit and a list of digits, so we need one more mapping step. We use `fold_left` to fold it into an integer. Since digits near the head of the list are left digits to the left, multiplying the digit by 10 and adding a right digit forms the final integer, which we then map to an enum.",
      "source_content_hash": "cb17329035e3ccbeb95886d862a8360f8ff35276f072e79bf32c66f34a1ec39d",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "最後に、整数の字句解析器を構築します。整数はゼロ、または非ゼロの数字で始まりその後に任意の数の数字が続くものです。まず3つのヘルパー・パーサーを構築します。最初のパーサーは文字`0`にマッチし、それを数値のゼロにマップします。次の2つのパーサーはそれぞれ`1-9`と`0-9`にマッチします。ここではUTFエンコーディングの範囲を使用して判定し、UTFの数字は0から9まで順序付けられているため、文字のエンコーディングと`0`のエンコーディングの差を計算して対応する数値を取得します。最後に、構文規則に従ってコンビネーターを使用してパーサーを構築します。11行目と12行目に示すように、規則を正確に反映しています。ただし、非ゼロ数字と任意の数の数字は単に数字と数字のリストのタプルを形成するため、さらに1つのマッピングステップが必要です。`fold_left`を使用して整数に畳み込みます。リストの先頭に近い数字が左側の桁になるため、数字を10倍して右側の数字を加算することで最終的な整数を形成し、それをenumにマップします。"
      }
    },
    {
      "segment_id": "d58c5dc0",
      "source_content": "```moonbit\n// Convert characters to integers via encoding\nlet zero: Lexer[Int] =\n  pchar(fn { ch => ch == '0' }).map(fn { _ => 0 })\nlet one_to_nine: Lexer[Int] =\n  pchar(fn { ch => ch.to_int() >= 0x31 && ch.to_int() <= 0x39 },).map(fn { ch => ch.to_int() - 0x30 })\nlet zero_to_nine: Lexer[Int] =\n  pchar(fn { ch => ch.to_int() >= 0x30 && ch.to_int() <= 0x39 },).map(fn { ch => ch.to_int() - 0x30 })\n\n// number = %x30 / (%x31-39) *(%x30-39)\nlet value : Lexer[Token] = zero.or(\n  one_to_nine.and(zero_to_nine.many()).map( // (Int, @immut/list.T[Int])\n    fn { (i, ls) => ls.fold_left(fn { i, j => i * 10 + j }, init=i) },\n  ),\n).map(Token::Value)\n```",
      "source_content_hash": "b76c9d65b680ab1a37cc46d116a84f7787fd1003ea06eb5e300558a46ea178aa",
      "node_type": "code",
      "translatable": false,
      "translations": {}
    },
    {
      "segment_id": "d636abf8",
      "source_content": "We're now just one step away from finishing lexical analysis: analyzing the entire input stream. There may be whitespaces in between tokens, so we allow arbitrary lengths of whitespaces after defining the number or symbol in line 2. We map and discard the second value in the tuple representing spaces, and may repeat the entire parser an arbitrary number of times. Finally, we can split a string into minus signs, numbers, plus signs, parentheses, etc. However, this output stream doesn't follow the syntax rules of arithmetic expressions. For this, we will need syntax analysis.",
      "source_content_hash": "d3320766460fe1a8621db506bce689a3b9fd8eaf286ffe55aeba39c2a4fc5d55",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "字句解析を完了するまであと一歩です：入力ストリーム全体を解析します。トークンの間に空白が存在する可能性があるため、2行目で数字や記号を定義した後に任意の長さの空白を許可します。空白を表すタプルの2番目の値をマップして破棄し、パーサー全体を任意の回数繰り返すことができます。最終的に、文字列をマイナス記号、数字、プラス記号、括弧などに分割できます。ただし、この出力ストリームは算術式の構文規則に従っていません。このため、構文解析が必要になります。"
      }
    },
    {
      "segment_id": "a1af15b5",
      "source_content": "```moonbit\nlet tokens : Lexer[@immut/list.T[Token]] =\n  value.or(symbol).and(whitespace.many())\n    .map(fn { (symbols, _) => symbols },) // Ignore whitespaces\n    .many()\n\ntest{\n  inspect!(tokens.parse(\"-10123+-+523 103    ( 5) )  \"), content=\"Some((@list.of([Minus, Value(10123), Plus, Minus, Plus, Value(523), Value(103), LParen, Value(5), RParen, RParen]), \\\"\\\"))\")\n}\n```",
      "source_content_hash": "c3657a7b87d8ac0ec68a24b63b9c5af63289dd88b92a21076902f4effbe771b0",
      "node_type": "code",
      "translatable": false,
      "translations": {}
    },
    {
      "segment_id": "61282bab",
      "source_content": "## Syntax Analysis",
      "source_content_hash": "fc2146a904a12baeac6e719e6182311a2475852b6bed4480ebfd729cd6e5d953",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## 構文解析"
      }
    },
    {
      "segment_id": "4e2e17c5",
      "source_content": "In the last example, we converted a string into a stream of tokens, discarded unimportant whitespaces, and split the string into meaningful enums. Now we will analyze whether the token stream is syntactically valid in terms of arithmetic expressions. As a simple example, the parentheses in an expression should be paired and should close in the correct order. We defined a simple syntax rule in the following code snippet. An arithmetic expression can be a single number, two arithmetic expressions carrying out an operation, or an expression surrounded by parentheses. We aim to convert a token stream into an abstract syntax tree like the one shown below. For the expression `1 + (1 - 5)`, the root node is a plus sign, representing the last operation executed. It means adding 1 to the expression on the right side. The right subtree contains a minus sign with integers 1 and 5, meaning 1 minus 5. The parentheses mean that it is executed earlier, so it's deeper down in the expression tree. Similarly, for the expression `(1 - 5) * 5`, the first calculation executed is the subtraction inside the parentheses, and then the multiplication.",
      "source_content_hash": "84e238ccb90fe0338dba7234fae885e5580b18e82f82b3fff27985920cb25fac",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "前の例では、文字列をトークンのストリームに変換し、重要でない空白を破棄して、文字列を意味のあるenumに分割しました。ここでは、トークンストリームが算術式として構文的に有効かどうかを分析します。簡単な例として、式内の括弧は対になっており、正しい順序で閉じられる必要があります。以下のコードスニペットで簡単な構文規則を定義しました。算術式は、単一の数字、演算を実行する2つの算術式、または括弧で囲まれた式のいずれかです。トークンストリームを以下のような抽象構文木に変換することを目指しています。式`1 + (1 - 5)`の場合、ルートノードはプラス記号で、最後に実行される演算を表します。これは1を右側の式に加算することを意味します。右側のサブツリーにはマイナス記号と整数1および5が含まれ、1から5を引くことを意味します。括弧はそれがより早く実行されることを意味するため、式ツリーのより深い位置にあります。同様に、式`(1 - 5) * 5`の場合、最初に実行される計算は括弧内の減算で、その後に乗算が実行されます。"
      }
    },
    {
      "segment_id": "bea1601a",
      "source_content": "```abnf\nexpression = Value / \"(\" expression \")\"\nexpression =/ expression \"+\" expression / expression \"-\" expression\nexpression =/ expression \"*\" expression / expression \"/\" expression\n```",
      "source_content_hash": "0cf5a52fb483c3a9a3968624119976d6dc5c9773cb0acb8404b4ca470c088bf5",
      "node_type": "code",
      "translatable": false,
      "translations": {}
    },
    {
      "segment_id": "6fad5db5",
      "source_content": "![](/pics/ast-example.drawio.webp)",
      "source_content_hash": "b52d8297913b277117ecf77cbd9c030813df2b4911d02a032549a5b5d30cffab",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "![](/pics/ast-example.drawio.webp)"
      }
    },
    {
      "segment_id": "c5e0bb3d",
      "source_content": "However, our syntax rules have some issues since it doesn't differentiate the precedence levels. For instance, `a + b * c` should be interpreted as `a` plus the product of `b` and `c`, but according to the current syntax rules, the sum of `a` and `b` multiplied by `c` is also valid, which introduces ambiguity. It also doesn't show associativity. Arithmetic operators should be left-associative, meaning `a + b + c` should be interpreted as `a` plus `b`, then adding `c`. However, the current syntax also allows adding `a` to the sum of `b` and `c`. So, we need to adjust the syntax rules for layering.",
      "source_content_hash": "8cedeb9dad7c9106e99c0061eff4018583b0ac6465f25fae892e717ac789a01f",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ただし、この構文規則にはいくつかの問題があります。優先順位を区別していないためです。例えば、`a + b * c`は`a`に`b`と`c`の積を加えたものと解釈されるべきですが、現在の構文規則では`a`と`b`の和に`c`を掛けたものも有効であり、曖昧さが生じます。また、結合性も示していません。算術演算子は左結合であるべきで、`a + b + c`は`a`に`b`を加え、その後`c`を加えると解釈されるべきです。しかし、現在の構文では`a`に`b`と`c`の和を加えることも許可されています。したがって、階層化のために構文規則を調整する必要があります。"
      }
    },
    {
      "segment_id": "f2c67ca5",
      "source_content": "The modified syntax rules are split into three parts. The first one is `atomic`, which is either an integer or an expression within parentheses. The second one is `atomic`, or `combine` multiplied or divided by `atomic`. The third one is `combine`, or `expression` plus or minus `combine`. The purpose of splitting into three rules is to differentiate operator precedence, while the left recursion in a single rule is to reflect left associativity. However, our parser can't handle left recursion. When trying to pattern match the left-recursive rule, it will first try to match the same rule on the side of the operator, which leads to recursion and prevents making progress. Still, this is just a limitation of our parser. Bottom-up parsers can handle left recursion, please refer to the recommended readings if you are interested.",
      "source_content_hash": "8eca5d0b05827903a3864dfb0f60b2c268b2c950bbf9b07d61fd2c3e722d8056",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "修正された構文規則は3つの部分に分割されます。最初の部分は`atomic`で、整数か括弧内の式のいずれかです。2番目の部分は`atomic`、または`combine`と`atomic`の乗算・除算です。3番目の部分は`combine`、または`expression`と`combine`の加算・減算です。これら3つの規則に分割する目的は演算子の優先順位を区別するためで、単一規則内の左再帰は左結合性を反映するためのものです。しかし、私たちのパーサーは左再帰を処理できません。左再帰規則をパターンマッチングしようとすると、演算子の側で同じ規則を最初にマッチしようとするため、再帰が発生して処理が進まなくなります。これはあくまで私たちのパーサーの制限です。ボトムアップパーサーは左再帰を処理できますので、興味があれば推薦図書を参照してください。"
      }
    },
    {
      "segment_id": "46dadadd",
      "source_content": "```abnf\natomic     = Value / \"(\" expression \")\"\ncombine    = atomic  /    combine \"*\" atomic  /    combine \"/\" atomic\nexpression = combine / expression \"+\" combine / expression \"-\" combine\n```",
      "source_content_hash": "51d591575d22dfab31bb84793fe360de5a97a1f85e9fc49a6f8c767bc9476a07",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_46dadadd"
      }
    },
    {
      "segment_id": "e6ac016e",
      "source_content": "Our modified syntax rules eliminate recursion but require additional processing for mapping. In short, we define the abstract syntax tree, and an expression can be an integer or a result of arithmetic operation expressions as mentioned before.",
      "source_content_hash": "7a143e177cf98ed8fe115bdee6b73a6d45daa8725b7a3e5a16aaf7df73e71b8c",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "修正された構文規則は再帰を排除していますが、マッピングのための追加処理が必要です。簡単に言えば、抽象構文木を定義し、式は前述の通り整数か算術演算式の結果のいずれかになります。"
      }
    },
    {
      "segment_id": "5b2e44c7",
      "source_content": "```abnf\natomic     = Value / \"(\" expression \")\"\ncombine    = atomic  *( (\"*\" / \"/\") atomic)\nexpression = combine *( (\"+\" / \"-\") combine)\n```",
      "source_content_hash": "bbfa9c89c1e13ec0fbd8b94278de7f19da82cd145ea5a1ed937061da2ce6f5e7",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_5b2e44c7"
      }
    },
    {
      "segment_id": "ff27b4c2",
      "source_content": "```moonbit\nenum Expression {\n  Number(Int)\n  Plus(Expression, Expression)\n  Minus(Expression, Expression)\n  Multiply(Expression, Expression)\n  Divide(Expression, Expression)\n}\n```",
      "source_content_hash": "9bd7996460f1a294623ef0e9c7dfeec47955846adf73b5c658397ca769a34453",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_ff27b4c2"
      }
    },
    {
      "segment_id": "950e5c6c",
      "source_content": "### Syntax Parsing",
      "source_content_hash": "9fc18566894577161a6f1b2a8ff4db78fc57d88520f0b07ad12fadbe725d584b",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "### 構文解析"
      }
    },
    {
      "segment_id": "49be422b",
      "source_content": "Let's define a syntax parser similar to the previous definitions, except that the input is now a list of tokens instead of a string. Most combinators are like the previous ones and can be implemented similarly. The challenge is how to define mutually recursive syntax parsers since `atomic` references `expression`, and `expression` depends on `combine`, which in turn depends on `atomic`. To solve this problem, we offer two solutions: deferring the definition or recursive functions.",
      "source_content_hash": "3c354d645981e6d2ce3254b257991841fc1be656c1b621926371a7ecfef94326",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "以前の定義と同様に構文パーサーを定義しましょう。ただし、入力は文字列ではなくトークンのリストになります。ほとんどのコンビネータは以前のものと同様で、同じように実装できます。課題は相互再帰的な構文パーサーをどう定義するかです。`atomic`は`expression`を参照し、`expression`は`combine`に依存し、`combine`は再び`atomic`に依存するからです。この問題を解決するために、2つの解決策を提供します：定義の遅延または再帰関数です。"
      }
    },
    {
      "segment_id": "5ad8d2b2",
      "source_content": "```moonbit\ntype Parser[V] (@immut/list.T[Token]) -> Option[(V, @immut/list.T[Token])]\n\nfn parse[V](self : Parser[V], tokens : @immut/list.T[Token]) -> Option[(V, @immut/list.T[Token])] {\n  (self.0)(tokens)\n}\n```",
      "source_content_hash": "7df47082310b592deb6b94fc95fd08a49c621930ae9b677acad651601d4289a7",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_5ad8d2b2"
      }
    },
    {
      "segment_id": "263073b1",
      "source_content": "### Recursive Definition",
      "source_content_hash": "56256c19a35880edff8230e13e221fd294437c4961cb0281801cfc2544de2997",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "### 再帰的定義"
      }
    },
    {
      "segment_id": "d119b75c",
      "source_content": "Deferring the definition involves first defining a reference with a default value. After defining the dependent combinators, the reference value is updated so that it retrieves the most recent value during computation. Here, we define a simple `ref` function that treats the reference as a normal parser and only fetches the current value during computation.",
      "source_content_hash": "8cd67c049d8d4acd87d9a73ee8469d80a80ffd0a9c4942f822a14ea8e20b599c",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "定義の遅延は、まずデフォルト値で参照を定義することを含みます。依存するコンビネータを定義した後、参照値を更新して、計算時に最新の値を取得できるようにします。ここでは、参照を通常のパーサーとして扱い、計算時にのみ現在の値を取得する単純な`ref`関数を定義します。"
      }
    },
    {
      "segment_id": "bce471ae",
      "source_content": "```moonbit\nfn Parser::ref[Value](ref: Ref[Parser[Value]]) -> Parser[Value] {\n  Parser(fn(input) {\n    ref.val.parse(input)\n  })\n}\n```",
      "source_content_hash": "8687a73f320b8c7ea6194638cab28b55049a662ad7d5926dccc3e9fa5be4d39f",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_bce471ae"
      }
    },
    {
      "segment_id": "6dc3e423",
      "source_content": "Afterward, we use the delayed definition to parse expressions. We construct a reference to the `expression` rule with a default value that will always fail to parse in any situation. Then, we define a parser for the first rule, `atomic`, using the series of combinators we defined earlier including `ref`, and following the syntax rules. The parser implementations for the left and right parentheses are relatively simple and we will not show any details here. Next, we define a parser for the second rule similarly. After defining the second rule, we update the value in the initial reference with the actual parser. Finally, we return the content of the actual parser. Since the content has been updated, we can directly access the value stored.",
      "source_content_hash": "78d4305ae3b37ed165bc933ea8db2f3f52719aa4c1ba790c48a6feeb188ea11a",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "その後、遅延定義を使用して式を解析します。`expression`規則への参照を構築し、どのような状況でも解析に失敗するデフォルト値を設定します。次に、`ref`を含む以前に定義した一連のコンビネータを使用し、構文規則に従って最初の規則`atomic`のパーサーを定義します。左右の括弧のパーサー実装は比較的単純で、ここでは詳細を示しません。次に、同様に2番目の規則のパーサーを定義します。2番目の規則を定義した後、初期参照の値を実際のパーサーで更新します。最後に、実際のパーサーの内容を返します。内容は更新されているので、保存された値に直接アクセスできます。"
      }
    },
    {
      "segment_id": "ab152d31",
      "source_content": "```moonbit no-check\nfn parser() -> Parser[Expression] {\n  // First define an empty reference\n  let expression_ref : Ref[Parser[Expression]] = { val : Parser(fn{ _ => None }) }\n\n  // atomic = Value / \"(\" expression \")\"\n  let atomic =  // Use the reference for the definition\n    (lparen.and(ref(expression_ref)).and(rparen).map(fn { ((_, expr), _) => expr}))\n      .or(number)\n\n  // combine = atomic *( (\"*\" / \"/\") atomic)\n  let combine = atomic.and(multiply.or(divide).and(atomic).many()).map(fn {\n    ...\n  })\n\n  // expression = combine *( (\"+\" / \"-\") combine)\n  expression_ref.val = combine.and(plus.or(minus).and(combine).many()).map(fn {\n    ...\n  })\n  ref(expression_ref)\n}\n```",
      "source_content_hash": "50c211c1cb6d025ea067626b66a45c80716f5b375b3ef9c648688bdb4bcd47af",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_ab152d31"
      }
    },
    {
      "segment_id": "216a8ee1",
      "source_content": "The concept of recursive functions is similar. Our parser is essentially a function wrapped in another type to distinguish it and make it easier to add methods. Therefore, we can define three mutually recursive functions, as shown in lines 4, 10, and 11. Then, in line 6, we construct the corresponding parser using the function. Finally, we return the parser corresponding to the function. At this point, we've completed our parser. We can use this parser to parse the token stream we previously analyzed into its corresponding abstract syntax tree. We can pattern-match to compute the result, but there are other available options to evaluate expressions.",
      "source_content_hash": "b6ba8f44f43fe70b7077a2e374f9707d3e1d0657ca960b267df19aacd54a0948",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "再帰関数の概念も同様です。私たちのパーサーは本質的に、区別しやすくメソッドを追加しやすいように別の型でラップされた関数です。したがって、4行目、10行目、11行目に示すように、3つの相互再帰関数を定義できます。次に、6行目で関数に対応するパーサーを構築します。最後に、関数に対応するパーサーを返します。これでパーサーが完成しました。このパーサーを使用して、以前に解析したトークンストリームを対応する抽象構文木に解析できます。結果を計算するためにパターンマッチングできますが、式を評価するための他のオプションもあります。"
      }
    },
    {
      "segment_id": "b7c66f56",
      "source_content": "```moonbit no-check\nfn recursive_parser() -> Parser[Expression] {\n  // Define mutually recursive functions\n  // atomic = Value / \"(\" expression \")\"\n  fn atomic(tokens: @immut/list.T[Token]) -> Option[(Expression, @immut/list.T[Token])] {\n    lparen.and(\n      Parser(expression) // Reference function\n    ).and(rparen).map(fn { ((_, expr), _) => expr})\n      .or(number).parse(tokens)\n  }\n  fn combine(tokens: @immut/list.T[Token]) -> Option[(Expression, @immut/list.T[Token])] { ... }\n  fn expression(tokens: @immut/list.T[Token]) -> Option[(Expression, @immut/list.T[Token])] { ... }\n\n  // Return the parser represented by the function\n  Parser(expression)\n}\n```",
      "source_content_hash": "b99b24cca876ca0b226e99bc420c66760773b6ed6155dd769f55577324add87a",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_b7c66f56"
      }
    },
    {
      "segment_id": "f3fc8281",
      "source_content": "### Beyond the Syntax Tree: Tagless Final",
      "source_content_hash": "69ec3ad46badc72327bdcb715d32907f6d081e797c287ef5e3b82c82e036b021",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "### 構文木を超えて：タグレスファイナル"
      }
    },
    {
      "segment_id": "60e22db4",
      "source_content": "Our previous approach is to generate an abstract syntax tree and then parse it. This is like labeling because we used enums. Here, we introduce Tagless Final as another approach to evaluate expressions without building an abstract syntax tree. This is possible thanks to the interfaces in MoonBit. We abstract the behavior through `trait`. An expression can be constructed from an integer and there can be arithmetic operations between two expressions. Based on this, we defined an interface. An interface comes with implementations, and different implementations of the interface correspond to different interpretations of the behavior or semantics.",
      "source_content_hash": "99ac7a77e910a9eb1fa802b2837f4f8b368df63d46db0457515eca5ccafcbdba",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "以前のアプローチは抽象構文木を生成してから解析するものでした。これは列挙型を使用したため、ラベル付けのようなものです。ここでは、タグレスファイナルを導入し、抽象構文木を構築せずに式を評価する別のアプローチを紹介します。これはMoonBitのインターフェースのおかげで可能です。`trait`を通じて動作を抽象化します。式は整数から構築でき、2つの式間で算術演算が可能です。これに基づいてインターフェースを定義しました。インターフェースには実装が伴い、インターフェースの異なる実装は動作や意味論の異なる解釈に対応します。"
      }
    },
    {
      "segment_id": "d0a5d100",
      "source_content": "```moonbit\ntrait Expr {\n  number(Int) -> Self\n  op_add(Self, Self) -> Self\n  op_sub(Self, Self) -> Self\n  op_mul(Self, Self) -> Self\n  op_div(Self, Self) -> Self\n}\n```",
      "source_content_hash": "439c65cea009b999260a5930f24982144091c1bece451597ebbba9864877fc90",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_d0a5d100"
      }
    },
    {
      "segment_id": "d1996877",
      "source_content": "We use behavioral abstractions to modify our parser. For integers, we don't construct an enum typed integer but instead use the `number` method of the interface. For arithmetic operations, we use the operators provided in the interface instead of the enum constructors in the mapping function. Lastly, we combine lexical and syntax parsing to produce a parser that processes a string into the final result. Note that we don't specify the parsing type, any interface compatible with the expression will work.",
      "source_content_hash": "b67f43f846b07fa160f875b3af4f90723eca10ec21fa04aefd62040b6b45aa8e",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "パーサーの動作を変更するために、振る舞いの抽象化を使用します。整数の場合、enum型の整数を構築する代わりに、インターフェースの`number`メソッドを使用します。算術演算では、マッピング関数内でenumコンストラクタの代わりにインターフェースで提供される演算子を使用します。最後に、字句解析と構文解析を組み合わせて、文字列を最終結果に処理するパーサーを生成します。解析タイプを指定しないことに注意してください。式と互換性のある任意のインターフェースが機能します。"
      }
    },
    {
      "segment_id": "d96a088c",
      "source_content": "```moonbit no-check\nfn recursive_parser[E : Expr]() -> Parser[E] {\n  let number : Parser[E] = ptoken(fn { Value(_) => true; _ => false})\n    .map(fn { Value(i) => E::number(i) }) // Use the abstract behavior\n\n  fn atomic(tokens: @immut/list.T[Token]) -> Option[(E, @immut/list.T[Token])] { ... }\n  // Convert to a * b * c * ... and a / b / c / ...\n  fn combine(tokens: @immut/list.T[Token]) -> Option[(E, @immut/list.T[Token])] { ... }\n  // Convert to a + b + c + ... and a - b - c - ...\n  fn expression(tokens: @immut/list.T[Token]) -> Option[(E, @immut/list.T[Token])] { ... }\n\n  Parser(expression)\n}\n// Put things together\nfn parse_string[E : Expr](str: String) -> Option[(E, String, @immut/list.T[Token])] {\n  let (token_list, rest_string) = match tokens.parse(str) {\n    Some(v) => v\n    None => return None\n  }\n  let (expr, rest_token) : (E, @immut/list.T[Token]) = match recursive_parser().parse(token_list) {\n    Some(v) => v\n    None => return None\n  }\n  Some(expr, rest_string, rest_token)\n}\n```",
      "source_content_hash": "8697cba4bb6b60586e69dac4c476e9760cb9244f61776551c36e77147f059c3a",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_d96a088c"
      }
    },
    {
      "segment_id": "a6efb0d1",
      "source_content": "Thus, we only need to define different implementations and specify which one to use in MoonBit. The former involves defining different methods for the data structure to meet the requirements of the interface, such as the `number` method in lines 4 and 5. The latter specifies the return type of functions to indicate the specific type parameter, as shown in lines 8 and 12. In line 8, we will obtain the expression tree constructed from enums, while in line 12 we can directly obtain the result. You can also add other interpretations, like converting an expression into a formatted string by removing extra parentheses and whitespaces.",
      "source_content_hash": "fbb85a8083b26c3fade7c75a9ae44733346a54aebd632e11696d4bc12df0a6fe",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "したがって、異なる実装を定義し、MoonBitでどれを使用するかを指定するだけで済みます。前者は、データ構造に対してインターフェースの要件を満たすために異なるメソッドを定義することを含みます（4行目と5行目の`number`メソッドなど）。後者は、特定の型パラメータを示すために関数の戻り値の型を指定します（8行目と12行目に示されています）。8行目では、enumから構築された式ツリーを取得し、12行目では直接結果を取得できます。また、余分な括弧や空白を削除して式をフォーマットされた文字列に変換するなど、他の解釈を追加することもできます。"
      }
    },
    {
      "segment_id": "ac3ddfe5",
      "source_content": "```moonbit no-check\nenum Expression { ... } derive(Show) // Implementation of syntax tree\ntype BoxedInt Int derive(Show) // Implementation of integer\n// Other interface implementation methods omitted\nfn BoxedInt::number(i: Int) -> BoxedInt { BoxedInt(i) }\nfn Expression::number(i: Int) -> Expression { Number(i) }\n// Parse\ntest {\n  inspect!((parse_string(\"1 + 1 * (307 + 7) + 5 - 3 - 2\") :\n    Option[(Expression, String, @immut/list.T[Token])]), content=\n    #|Some((Minus(Minus(Plus(Plus(Number(1), Multiply(Number(1), Plus(Number(307), Number(7)))), Number(5)), Number(3)), Number(2)), \"\", @immut/list.T::[]))\n  ) // Get the syntax tree\n  inspect!((parse_string(\"1 + 1 * (307 + 7) + 5 - 3 - 2\") :\n    Option[(BoxedInt, String, @immut/list.T[Token])]), content=\n    #|Some((BoxedInt(315), \"\", @immut/list.T::[]))\n  ) // Get the calculation result\n  }\n```",
      "source_content_hash": "858f45af57ee963b32ee7a6543fe4cbf9608fcc4d539f6a9b166f028c45dee84",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_ac3ddfe5"
      }
    },
    {
      "segment_id": "fb79182e",
      "source_content": "## Summary",
      "source_content_hash": "30ac03ff33731529441be8fbe52a3bd0d4c5ec830e806d54692168ebb7f98ada",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## まとめ"
      }
    },
    {
      "segment_id": "4667c154",
      "source_content": "In summary, we presented a parser in this lecture. We introduced the concepts of lexical and syntax analysis, illustrated the definition and implementation of parser combinators, and expanded on the concept and implementation of Tagless Final. There are a few points to add. For arithmetic expressions, there's a simple algorithm called the Shunting Yard algorithm that calculates the value of the expression after splitting it into tokens. Syntax analysis/parsing is an important field in computer science, and understanding it thoroughly takes a lot of time. It's impossible to cover everything in just one lecture, so we only provided a brief introduction. Feel free to refer to lectures 1-8 of Stanford course CS143, the first five chapters of *Compilers: Principles, Techniques, and Tools*, or the first three chapters of *Modern Compiler Implementation*. Also, the latter two books are sometimes called the Dragon Book and Tiger Book because of their cover designs.",
      "source_content_hash": "8c1d57e27564ac6c4b2208b50333dd5c1687b81e5e44a4b964f750ca008c76b7",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "要約すると、この講義ではパーサーを紹介しました。字句解析と構文解析の概念を紹介し、パーサーコンビネータの定義と実装を説明し、Tagless Finalの概念と実装について拡張しました。追加すべき点がいくつかあります。算術式の場合、Shunting Yardアルゴリズムと呼ばれる単純なアルゴリズムがあり、式をトークンに分割した後に式の値を計算します。構文解析/パーシングはコンピュータサイエンスの重要な分野であり、完全に理解するには多くの時間がかかります。1回の講義ですべてを網羅することは不可能なので、簡単な紹介のみを行いました。スタンフォード大学のCS143コースの講義1-8、『コンパイラ―原理・技法・ツール』の最初の5章、または『現代コンパイラの実装』の最初の3章を参照してください。また、後者の2冊の本は、表紙のデザインからドラゴンブックとタイガーブックと呼ばれることもあります。"
      }
    },
    {
      "segment_id": "b6abd1ad",
      "source_content": "There's also an additional exercise: You might have noticed that the structure of string and token stream parsers is almost identical, so is it possible to abstract the strings and token streams to implement a parser combinator compatible with different streams? You're encouraged to think about it!",
      "source_content_hash": "20dd04ac1f571ae386e71945b4f69203fdc693f93e20baa1dbd5b2a8cc46ea2e",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "追加の演習問題もあります。文字列とトークンストリームのパーサーの構造がほぼ同一であることに気付いたかもしれません。では、異なるストリームと互換性のあるパーサーコンビネータを実装するために、文字列とトークンストリームを抽象化することは可能でしょうか？ぜひ考えてみてください！"
      }
    },
    {
      "segment_id": "4792de79",
      "source_content": "Lastly, we hope everyone notices the important modular programming thinking embodied in parsing combinators: building modules from small to large that can be combined together to build programs from simple to complex. By adopting modular thinking, we can manage complexity and discard irrelevant information. For instance, when we defined the parser at the end, we used combinators to basically replicate the syntax one-to-one, without worrying about the specific implementation of the parser. Only in this way can we build larger, maintainable programs with fewer bugs.",
      "source_content_hash": "63983c12087ff720f635bcc5fe669d756c202d4527610f2b8456758ba5d067e3",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "最後に、パーサーコンビネータに体現された重要なモジュールプログラミングの考え方に注目していただきたいと思います。小さいものから大きいものへとモジュールを構築し、単純なものから複雑なものへとプログラムを組み立てることができます。モジュール思考を採用することで、複雑さを管理し、無関係な情報を捨てることができます。たとえば、最後にパーサーを定義したとき、コンビネータを使用して構文をほぼ1対1で複製し、パーサーの具体的な実装を気にする必要はありませんでした。このようにしてのみ、より大きく、保守可能で、バグの少ないプログラムを構築できます。"
      }
    }
  ],
  "target_i18n_subpath": "docusaurus-plugin-content-docs/current/11-parser.md",
  "last_updated_timestamp": "2025-06-06T05:19:35.746505+00:00",
  "schema_version": "1.0",
  "translated_versions": {
    "ja": "599b2605fd3d9549bd467dd412df73b6b43ea50662b7347b557c7ddff7528a1c"
  }
}